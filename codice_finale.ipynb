{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25Ua53mVsuIL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import trange\n",
        "import os\n",
        "from PIL import Image\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset creation and preprocessing"
      ],
      "metadata": {
        "id": "5Xm2gTTJt2R0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "\n",
        "DATA_DIR = \"flowers\"\n",
        "x_paths = [DATA_DIR+\"/\"+i for i in sorted(os.listdir(DATA_DIR)) if not i.startswith(\".\")]\n",
        "data = []\n",
        "\n",
        "#resize images if necessary\n",
        "for image_path_i in trange(len(x_paths)):\n",
        "    image_path = x_paths[image_path_i]\n",
        "\n",
        "    try:\n",
        "        image_array = np.array(Image.open(image_path))\n",
        "        if len(image_array.shape) == 3:  # Check if the image has 3 dimensions\n",
        "            data.append(tf.image.resize(image_array, (128, 128)))\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image {image_path}: {str(e)}\")\n",
        "\n",
        "#create the splits\n",
        "data = np.array(data)\n",
        "print(data.shape)\n",
        "print(\"train/test/val split pt1\")\n",
        "X_train, X_test = train_test_split(data, test_size=0.2, random_state=1)\n",
        "print(\"train/test/val split pt2\")\n",
        "X_train, X_val = train_test_split(X_train, test_size=0.25, random_state=1)  # 0.25 x 0.8 = 0.2\n",
        "\n",
        "#save array into folder\n",
        "if not os.path.exists('dataset'):\n",
        "    os.makedirs('dataset')\n",
        "\n",
        "with open('dataset/X_train.npy', 'wb+') as f:\n",
        "    np.save(f, np.array(X_train))\n",
        "\n",
        "with open('dataset/X_val.npy', 'wb+') as f:\n",
        "    np.save(f, np.array(X_val))\n",
        "\n",
        "with open('dataset/X_test.npy', 'wb+') as f:\n",
        "    np.save(f, np.array(X_test))"
      ],
      "metadata": {
        "id": "jcHtFeg1txmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train definition"
      ],
      "metadata": {
        "id": "kjueRIfVuqs_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import trange\n",
        "import os\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "import keras.api._v2.keras as K\n",
        "import pickle\n",
        "import time"
      ],
      "metadata": {
        "id": "67mOQucKuw8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#collegamento drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Rv0PJx4muf9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dnn(X_train, X_val, model, name, batch_size=128, epochs=30, save_only_weights=False):\n",
        "    @tf.function\n",
        "    def train_step(x, y):\n",
        "        with tf.GradientTape() as tape:\n",
        "            preds = model(x, training=True)\n",
        "            loss_value = model.loss(y, preds)\n",
        "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "        model.optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "        return loss_value\n",
        "\n",
        "    def hue_shift(image):\n",
        "        hue_shift_value = tf.random.uniform([], -0.5, 0.5)\n",
        "        adjusted_image = tf.image.adjust_hue(image, hue_shift_value)\n",
        "        return adjusted_image\n",
        "\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, X_train))\n",
        "    train_dataset = train_dataset.map(lambda x, y: (hue_shift(x), y))\n",
        "\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, X_train)) \\\n",
        "        .map(lambda x, y: (tf.reduce_mean(x, axis=-1, keepdims=True), tf.cast(y, tf.float32) / 255.)) \\\n",
        "        .cache() \\\n",
        "        .shuffle(buffer_size=1024) \\\n",
        "        .batch(batch_size)\n",
        "\n",
        "    val_dataset = tf.data.Dataset.from_tensor_slices((X_val, X_val)) \\\n",
        "        .map(lambda x, y: (tf.reduce_mean(x, axis=-1, keepdims=True), tf.cast(y, tf.float32) / 255.)) \\\n",
        "        .cache() \\\n",
        "        .shuffle(buffer_size=1024) \\\n",
        "        .batch(batch_size)\n",
        "\n",
        "    losses_history = []\n",
        "    for epoch in range(epochs):\n",
        "        print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "        start_time = time.time()\n",
        "\n",
        "        train_losses = []\n",
        "        for step, (x_batch_train, y_batch_train) in enumerate(tqdm(train_dataset)):\n",
        "            train_losses.append(\n",
        "                train_step(x_batch_train, y_batch_train)\n",
        "            )\n",
        "\n",
        "        val_losses = []\n",
        "        for x_batch_val, y_batch_val in val_dataset:\n",
        "            val_losses.append(model.evaluate(x_batch_val, y_batch_val, verbose=0))\n",
        "\n",
        "        print(\"Train loss: %.4f\" % (float(np.mean(train_losses)),))\n",
        "        print(\"Validation loss: %.4f\" % (float(np.mean(val_losses)),))\n",
        "        print(\"Time taken: %.2fs\" % (time.time() - start_time))\n",
        "\n",
        "        losses_history.append([float(np.mean(train_losses)), float(np.mean(val_losses))])\n",
        "\n",
        "        if save_only_weights:\n",
        "            weights_dir = f\"drive/MyDrive/models/{name}\"\n",
        "            os.makedirs(weights_dir, exist_ok=True)\n",
        "            model.save_weights(f\"{weights_dir}/{epoch}\")\n",
        "        else:\n",
        "            model_dir = f\"drive/MyDrive/models/{name}\"\n",
        "            os.makedirs(model_dir, exist_ok=True)\n",
        "            model.save(f\"{model_dir}/{epoch}\")\n",
        "\n",
        "        history_dir = f\"drive/MyDrive/models/{name}\"\n",
        "        os.makedirs(history_dir, exist_ok=True)\n",
        "        with open(f\"{history_dir}/loss_history\", \"wb+\") as f:\n",
        "            pickle.dump(losses_history, f)\n",
        "\n",
        "    return losses_history\n",
        "\n",
        "\n",
        "def train_pix2pix(X_train, generator, discriminator, name, alpha=1.0, batch_size=32, epochs=30, save_only_weights=False):\n",
        "    @tf.function\n",
        "    def train_step(x, y):\n",
        "        generated_rgb_images = generator(x, training=False)\n",
        "        with tf.GradientTape() as tape:\n",
        "            preds_1 = discriminator([x, y])\n",
        "            preds_0 = discriminator([x, generated_rgb_images])\n",
        "            loss = tf.reduce_mean((1 - preds_1)**2) + tf.reduce_mean(preds_0**2)\n",
        "        grads = tape.gradient(loss, discriminator.trainable_weights)\n",
        "        discriminator.optimizer.apply_gradients(zip(grads, discriminator.trainable_weights))\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            generated_rgb_images = generator(x)\n",
        "            preds = discriminator([x, generated_rgb_images])\n",
        "            loss = generator.loss(y, generated_rgb_images) + alpha * (\n",
        "                -tf.reduce_mean(\n",
        "                    tf.math.log(preds + 1e-8)\n",
        "                )\n",
        "            )\n",
        "        grads = tape.gradient(loss, generator.trainable_weights)\n",
        "        generator.optimizer.apply_gradients(zip(grads, generator.trainable_weights))\n",
        "        return tf.reduce_mean(preds)\n",
        "\n",
        "    def hue_shift(image):\n",
        "        hue_shift_value = tf.random.uniform([], -0.5, 0.5)\n",
        "        adjusted_image = tf.image.adjust_hue(image, hue_shift_value)\n",
        "        return adjusted_image\n",
        "\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, X_train))\n",
        "    train_dataset = train_dataset.map(lambda x, y: (hue_shift(x), y))\n",
        "\n",
        "\n",
        "     #def contrast_adjust(image):\n",
        "      #contrast_factor = tf.random.uniform([], 0.5, 1.5)\n",
        "      #adjusted_image = tf.image.adjust_contrast(image, contrast_factor)\n",
        "      #return adjusted_image\n",
        "\n",
        "    #train_dataset = tf.data.Dataset.from_tensor_slices((X_train, X_train))\n",
        "    #train_dataset = train_dataset.map(lambda x, y: (contrast_adjust(x), y))\n",
        "\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, X_train)) \\\n",
        "        .map(lambda x, y: (tf.reduce_mean(x, axis=-1, keepdims=True), tf.cast(y, tf.float32) / 255.)) \\\n",
        "        .cache() \\\n",
        "        .shuffle(buffer_size=1024) \\\n",
        "        .batch(batch_size)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        disc_losses = []\n",
        "        print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "        start_time = time.time()\n",
        "\n",
        "        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "            disc_losses.append(train_step(x_batch_train, y_batch_train))\n",
        "            print(f\"\\r{step}/{train_dataset.cardinality().numpy()} - Avg disc pred: {np.mean(disc_losses[-3:])}\", end=\"\")\n",
        "\n",
        "        print(\"Time taken: %.2fs\" % (time.time() - start_time))\n",
        "\n",
        "        if save_only_weights:\n",
        "            generator.save_weights(f\"models/{name}/{epoch}/generator\")\n",
        "            discriminator.save_weights(f\"models/{name}/{epoch}/discriminator\")\n",
        "        else:\n",
        "            generator.save(f\"models/{name}/{epoch}/generator\")\n",
        "            discriminator.save(f\"models/{name}/{epoch}/discriminator\")"
      ],
      "metadata": {
        "id": "zDkIC68Auu2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Autoencoder"
      ],
      "metadata": {
        "id": "7DzQFCUfwuRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_autoencoder():\n",
        "    inputs = K.layers.Input(shape=(128, 128, 1))\n",
        "    x = K.layers.BatchNormalization()(inputs)\n",
        "    x = K.layers.Conv2D(32, 3, activation=\"linear\", padding=\"SAME\", use_bias=False)(x)\n",
        "    x = K.layers.BatchNormalization(axis=-1)(x)\n",
        "    x = K.layers.Activation(tf.nn.leaky_relu)(x)\n",
        "    x = K.layers.Conv2D(32, 3, activation=\"linear\", padding=\"SAME\", use_bias=False)(x)\n",
        "    x = K.layers.BatchNormalization(axis=-1)(x)\n",
        "    x = K.layers.Activation(tf.nn.leaky_relu)(x)\n",
        "    x = K.layers.Conv2D(32, 3, activation=\"linear\", padding=\"SAME\", use_bias=False)(x)\n",
        "    x = K.layers.BatchNormalization(axis=-1)(x)\n",
        "    x = K.layers.Activation(tf.nn.leaky_relu)(x)\n",
        "\n",
        "    x = K.layers.MaxPool2D(2)(x)\n",
        "\n",
        "    x = K.layers.Conv2D(64, 3, activation=\"linear\", padding=\"SAME\", use_bias=False)(x)\n",
        "    x = K.layers.BatchNormalization(axis=-1)(x)\n",
        "    x = K.layers.Activation(tf.nn.leaky_relu)(x)\n",
        "    x = K.layers.Conv2D(64, 3, activation=\"linear\", padding=\"SAME\", use_bias=False)(x)\n",
        "    x = K.layers.BatchNormalization(axis=-1)(x)\n",
        "    x = K.layers.Activation(tf.nn.leaky_relu)(x)\n",
        "    x = K.layers.Conv2D(64, 3, activation=\"linear\", padding=\"SAME\", use_bias=False)(x)\n",
        "    x = K.layers.BatchNormalization(axis=-1)(x)\n",
        "    x = K.layers.Activation(tf.nn.leaky_relu)(x)\n",
        "\n",
        "    x = K.layers.MaxPool2D(2)(x)\n",
        "\n",
        "    x = K.layers.Conv2D(128, 3, activation=\"linear\", padding=\"SAME\", use_bias=False)(x)\n",
        "    x = K.layers.BatchNormalization(axis=-1)(x)\n",
        "    x = K.layers.Activation(tf.nn.leaky_relu)(x)\n",
        "    x = K.layers.Conv2D(128, 3, activation=\"linear\", padding=\"SAME\", use_bias=False)(x)\n",
        "    x = K.layers.BatchNormalization(axis=-1)(x)\n",
        "    x = K.layers.Activation(tf.nn.leaky_relu)(x)\n",
        "    x = K.layers.Conv2D(128, 3, activation=\"linear\", padding=\"SAME\", use_bias=False)(x)\n",
        "    x = K.layers.BatchNormalization(axis=-1)(x)\n",
        "    x = K.layers.Activation(tf.nn.leaky_relu)(x)\n",
        "\n",
        "    x = K.layers.UpSampling2D(2)(x)\n",
        "\n",
        "    x = K.layers.Conv2D(64, 3, activation=\"linear\", padding=\"SAME\", use_bias=False)(x)\n",
        "    x = K.layers.BatchNormalization(axis=-1)(x)\n",
        "    x = K.layers.Activation(tf.nn.leaky_relu)(x)\n",
        "    x = K.layers.Conv2D(64, 3, activation=\"linear\", padding=\"SAME\", use_bias=False)(x)\n",
        "    x = K.layers.BatchNormalization(axis=-1)(x)\n",
        "    x = K.layers.Activation(tf.nn.leaky_relu)(x)\n",
        "    x = K.layers.Conv2D(64, 3, activation=\"linear\", padding=\"SAME\", use_bias=False)(x)\n",
        "    x = K.layers.BatchNormalization(axis=-1)(x)\n",
        "    x = K.layers.Activation(tf.nn.leaky_relu)(x)\n",
        "\n",
        "    x = K.layers.UpSampling2D(2)(x)\n",
        "\n",
        "    x = K.layers.Conv2D(32, 3, activation=\"linear\", padding=\"SAME\", use_bias=False)(x)\n",
        "    x = K.layers.BatchNormalization(axis=-1)(x)\n",
        "    x = K.layers.Activation(tf.nn.leaky_relu)(x)\n",
        "    x = K.layers.Conv2D(32, 3, activation=\"linear\", padding=\"SAME\", use_bias=False)(x)\n",
        "    x = K.layers.BatchNormalization(axis=-1)(x)\n",
        "    x = K.layers.Activation(tf.nn.leaky_relu)(x)\n",
        "    x = K.layers.Conv2D(3, 1, activation=\"sigmoid\", padding=\"SAME\", use_bias=True)(x)\n",
        "\n",
        "    autoencoder = K.models.Model(inputs=inputs, outputs=x)\n",
        "    return autoencoder\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    with open('drive/MyDrive/dataset/X_train.npy', 'rb') as f:\n",
        "        X_train = np.load(f)\n",
        "    with open('drive/MyDrive/dataset/X_val.npy', 'rb') as f:\n",
        "        X_val = np.load(f)\n",
        "    with open('drive/MyDrive/dataset/X_test.npy', 'rb') as f:\n",
        "        X_test = np.load(f)\n",
        "\n",
        "    autoencoder = create_autoencoder()\n",
        "\n",
        "    autoencoder.compile(\n",
        "        tf.optimizers.legacy.Adam(1e-3),\n",
        "        loss=tf.losses.MeanSquaredError()\n",
        "    )\n",
        "    train_dnn(X_train, X_val, autoencoder, \"autoencoder\", batch_size=16)\n"
      ],
      "metadata": {
        "id": "zlPQEUFZwvq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep neural network"
      ],
      "metadata": {
        "id": "T7cd-Pplw3jo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dnet():\n",
        "    inputs = K.layers.Input(shape=(128, 128, 1))\n",
        "    x = K.layers.BatchNormalization(axis=-1)(inputs)\n",
        "\n",
        "    for _ in range(6):\n",
        "        x = K.layers.Conv2D(64, 3, activation=\"linear\", padding=\"SAME\", use_bias=False)(x)\n",
        "        x = K.layers.BatchNormalization(axis=-1)(x)\n",
        "        x = K.layers.Activation(tf.nn.leaky_relu)(x)\n",
        "\n",
        "    x = K.layers.Conv2D(3, 3, activation=\"sigmoid\", padding=\"SAME\", use_bias=True)(x)\n",
        "\n",
        "    dnet = K.models.Model(inputs=inputs, outputs=x)\n",
        "    return dnet\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    with open('drive/MyDrive/dataset/X_train.npy', 'rb') as f:\n",
        "        X_train = np.load(f)\n",
        "    with open('drive/MyDrive/dataset/X_val.npy', 'rb') as f:\n",
        "        X_val = np.load(f)\n",
        "    with open('drive/MyDrive/dataset/X_test.npy', 'rb') as f:\n",
        "        X_test = np.load(f)\n",
        "\n",
        "    dnet = create_dnet()\n",
        "\n",
        "    dnet.compile(\n",
        "        optimizer=tf.optimizers.legacy.Adam(1e-3),\n",
        "        loss=tf.losses.MeanSquaredError()\n",
        "    )\n",
        "    train_dnn(X_train, X_val, dnet, \"dnet\", batch_size=16)\n"
      ],
      "metadata": {
        "id": "yKU_j2klw9Vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unet"
      ],
      "metadata": {
        "id": "C1bPpa8XxaeL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualLayer(K.layers.Layer):\n",
        "    def __init__(self, ffnn, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.ffnn = K.models.Sequential(ffnn)\n",
        "\n",
        "    def call(self, inputs, *args, **kwargs):\n",
        "        return tf.concat((self.ffnn(inputs), inputs), axis=-1)\n",
        "\n",
        "\n",
        "def create_unet():\n",
        "    inputs = K.layers.Input(shape=(128, 128, 1))\n",
        "    x = K.layers.BatchNormalization(axis=-1)(inputs)\n",
        "\n",
        "    # Encoder\n",
        "    for filters in [16, 32, 64, 128, 256]:\n",
        "        x = K.layers.Conv2D(filters, 3, activation=\"linear\", padding=\"SAME\", use_bias=False)(x)\n",
        "        x = K.layers.BatchNormalization(axis=-1)(x)\n",
        "        x = K.layers.Activation(tf.nn.leaky_relu)(x)\n",
        "\n",
        "        if filters != 256:\n",
        "            x = K.layers.MaxPool2D(2)(x)\n",
        "        else:\n",
        "            x = ResidualLayer([\n",
        "                K.layers.Conv2D(256, 1, activation=\"linear\", padding=\"SAME\", use_bias=False),\n",
        "                K.layers.BatchNormalization(axis=-1),\n",
        "                K.layers.Activation(tf.nn.leaky_relu),\n",
        "                K.layers.Conv2D(256, 3, activation=\"linear\", padding=\"SAME\", use_bias=False),\n",
        "                K.layers.BatchNormalization(axis=-1),\n",
        "                K.layers.Activation(tf.nn.leaky_relu),\n",
        "            ])(x)\n",
        "\n",
        "    # Decoder\n",
        "    for filters in [128, 64, 32, 16]:\n",
        "        x = K.layers.UpSampling2D(2)(x)\n",
        "        x = K.layers.Conv2D(filters, 3, activation=\"linear\", padding=\"SAME\", use_bias=False)(x)\n",
        "        x = K.layers.BatchNormalization(axis=-1)(x)\n",
        "        x = K.layers.Activation(tf.nn.leaky_relu)(x)\n",
        "        x = K.layers.Conv2D(filters, 3, activation=\"linear\", padding=\"SAME\", use_bias=False)(x)\n",
        "        x = K.layers.BatchNormalization(axis=-1)(x)\n",
        "        x = K.layers.Activation(tf.nn.leaky_relu)(x)\n",
        "\n",
        "    # Output Layer\n",
        "    x = K.layers.UpSampling2D(2)(x)\n",
        "    x = K.layers.Conv2D(3, 3, activation=\"sigmoid\", padding=\"SAME\")(x)\n",
        "\n",
        "    unet = K.models.Model(inputs=inputs, outputs=x)\n",
        "    return unet\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    with open('drive/MyDrive/dataset/X_train.npy', 'rb') as f:\n",
        "        X_train = np.load(f)\n",
        "    with open('drive/MyDrive/dataset/X_val.npy', 'rb') as f:\n",
        "        X_val = np.load(f)\n",
        "    with open('drive/MyDrive/dataset/X_test.npy', 'rb') as f:\n",
        "        X_test = np.load(f)\n",
        "    unet = create_unet()\n",
        "    unet.compile(\n",
        "        optimizer=tf.optimizers.legacy.Adam(1e-3),\n",
        "        loss=tf.losses.MeanSquaredError()\n",
        "    )\n",
        "\n",
        "    train_dnn(X_train, X_val, unet, \"unet\", batch_size=16, save_only_weights=False)"
      ],
      "metadata": {
        "id": "uXBH_Q7OxhAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resnet"
      ],
      "metadata": {
        "id": "1unTHFiNx87f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualLayer(K.layers.Layer):\n",
        "    def __init__(self, ffnn, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.ffnn = K.models.Sequential(ffnn)\n",
        "\n",
        "    def call(self, inputs, *args, **kwargs):\n",
        "        return self.ffnn(inputs) + inputs\n",
        "\n",
        "\n",
        "def create_resnet():\n",
        "    inputs = K.layers.Input(shape=(128, 128, 1))\n",
        "    x = K.layers.BatchNormalization(axis=-1)(inputs)\n",
        "    x = K.layers.Conv2D(256, 1, activation=\"linear\", padding=\"SAME\")(x)\n",
        "\n",
        "    # Initial Residual Block\n",
        "    x = ResidualLayer([\n",
        "        K.layers.BatchNormalization(axis=-1),\n",
        "        K.layers.Activation(tf.nn.leaky_relu),\n",
        "        K.layers.Conv2D(32, 1, activation=\"linear\", padding=\"SAME\", use_bias=False),\n",
        "        K.layers.BatchNormalization(axis=-1),\n",
        "        K.layers.Activation(tf.nn.leaky_relu),\n",
        "        K.layers.Conv2D(32, 3, activation=\"linear\", padding=\"SAME\", use_bias=False),\n",
        "        K.layers.BatchNormalization(axis=-1),\n",
        "        K.layers.Activation(tf.nn.leaky_relu),\n",
        "        K.layers.Conv2D(256, 1, activation=\"linear\", padding=\"SAME\", use_bias=False),\n",
        "    ])(x)\n",
        "\n",
        "    # Residual Blocks\n",
        "    for _ in range(20):\n",
        "        x = ResidualLayer([\n",
        "            K.layers.BatchNormalization(axis=-1),\n",
        "            K.layers.Activation(tf.nn.leaky_relu),\n",
        "            K.layers.Conv2D(32, 1, activation=\"linear\", padding=\"SAME\", use_bias=False),\n",
        "            K.layers.BatchNormalization(axis=-1),\n",
        "            K.layers.Activation(tf.nn.leaky_relu),\n",
        "            K.layers.Conv2D(32, 3, activation=\"linear\", padding=\"SAME\", use_bias=False),\n",
        "            K.layers.BatchNormalization(axis=-1),\n",
        "            K.layers.Activation(tf.nn.leaky_relu),\n",
        "            K.layers.Conv2D(256, 1, activation=\"linear\", padding=\"SAME\", use_bias=False),\n",
        "        ])(x)\n",
        "\n",
        "    # Finalization\n",
        "    x = K.layers.BatchNormalization(axis=-1)(x)\n",
        "    x = K.layers.Activation(tf.nn.leaky_relu)(x)\n",
        "    x = K.layers.Conv2D(3, 3, activation=\"sigmoid\", padding=\"SAME\")(x)\n",
        "\n",
        "    resnet = K.models.Model(inputs=inputs, outputs=x)\n",
        "    return resnet\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    with open('drive/MyDrive/dataset/X_train.npy', 'rb') as f:\n",
        "        X_train = np.load(f)\n",
        "    with open('drive/MyDrive/dataset/X_val.npy', 'rb') as f:\n",
        "        X_val = np.load(f)\n",
        "    with open('drive/MyDrive/dataset/X_test.npy', 'rb') as f:\n",
        "        X_test = np.load(f)\n",
        "\n",
        "    resnet = create_resnet()\n",
        "\n",
        "    resnet.compile(\n",
        "        optimizer=tf.optimizers.legacy.Adam(1e-3),\n",
        "        loss=tf.losses.MeanSquaredError()\n",
        "    )\n",
        "    train_dnn(X_train, X_val, resnet, \"resnet\", batch_size=8)\n"
      ],
      "metadata": {
        "id": "pdtKipCnyAA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dense net"
      ],
      "metadata": {
        "id": "ln4DxLzXybns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualLayer(K.layers.Layer):\n",
        "    def __init__(self, ffnn, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.ffnn = K.models.Sequential(ffnn)\n",
        "\n",
        "    def call(self, inputs, *args, **kwargs):\n",
        "        return tf.concat((self.ffnn(inputs), inputs), axis=-1)\n",
        "\n",
        "\n",
        "def create_dense_net():\n",
        "    inputs = K.layers.Input(shape=(128, 128, 1))\n",
        "    x = K.layers.Conv2D(filters=256, kernel_size=1, padding=\"SAME\", activation=\"linear\")(inputs)\n",
        "    x = K.layers.BatchNormalization()(x)\n",
        "    x = K.layers.Activation(tf.nn.leaky_relu)(x)\n",
        "    concatenated_inputs = x\n",
        "    for f in [128] * 7:\n",
        "        x = K.layers.Conv2D(filters=64, kernel_size=1, padding=\"SAME\", activation=\"linear\")(concatenated_inputs)\n",
        "        x = K.layers.BatchNormalization()(x)\n",
        "        x = K.layers.Activation(tf.nn.leaky_relu)(x)\n",
        "        x = K.layers.Conv2D(filters=f, kernel_size=4, padding=\"SAME\", activation=\"linear\")(x)\n",
        "        x = K.layers.BatchNormalization()(x)\n",
        "        x = K.layers.Activation(tf.nn.leaky_relu)(x)\n",
        "        concatenated_inputs = K.layers.Concatenate()([concatenated_inputs, x])\n",
        "\n",
        "    x = K.layers.Conv2D(filters=64, kernel_size=1, padding=\"SAME\", activation=\"linear\")(concatenated_inputs)\n",
        "    x = K.layers.BatchNormalization()(x)\n",
        "    x = K.layers.Activation(tf.nn.leaky_relu)(x)\n",
        "    x = K.layers.Conv2D(3, 3, activation=\"sigmoid\", padding=\"SAME\")(x)\n",
        "    dense_net = K.models.Model(inputs=[inputs], outputs=[x])\n",
        "    return dense_net\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    with open('drive/MyDrive/dataset/X_train.npy', 'rb') as f:\n",
        "        X_train = np.load(f)\n",
        "    with open('drive/MyDrive/dataset/X_val.npy', 'rb') as f:\n",
        "        X_val = np.load(f)\n",
        "    with open('drive/MyDrive/dataset/X_test.npy', 'rb') as f:\n",
        "        X_test = np.load(f)\n",
        "\n",
        "    dense_net = create_dense_net()\n",
        "\n",
        "    dense_net.compile(\n",
        "        optimizer=tf.optimizers.legacy.Adam(1e-3),\n",
        "        loss=tf.losses.MeanSquaredError()\n",
        "    )\n",
        "\n",
        "    train_dnn(X_train, X_val, dense_net, \"dense_net\", batch_size=16, save_only_weights=False)\n"
      ],
      "metadata": {
        "id": "AiKkMrOAydsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pix2pix"
      ],
      "metadata": {
        "id": "Z2ygVdJ1zLCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualLayer(K.layers.Layer):\n",
        "    def __init__(self, ffnn, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.ffnn = K.models.Sequential(ffnn)\n",
        "\n",
        "    def call(self, inputs, *args, **kwargs):\n",
        "        return tf.concat((self.ffnn(inputs), inputs), axis=-1)\n",
        "\n",
        "\n",
        "def create_discriminator_p2p():\n",
        "    inputs = K.layers.Input(shape=(128, 128, 3))\n",
        "    conditioning = K.layers.Input(shape=(128, 128, 1))\n",
        "    x = K.layers.Concatenate(axis=-1)([inputs, conditioning])\n",
        "    x = K.layers.BatchNormalization(axis=-1)(x)\n",
        "    x = K.layers.Conv2D(512, 3, activation=\"linear\", use_bias=False)(x)\n",
        "    x = K.layers.BatchNormalization(axis=-1)(x)\n",
        "    x = K.layers.Activation(tf.nn.leaky_relu)(x)\n",
        "    x = K.layers.Conv2D(128, 1, activation=\"linear\", use_bias=False)(x)\n",
        "    x = K.layers.BatchNormalization(axis=-1)(x)\n",
        "    x = K.layers.Activation(tf.nn.leaky_relu)(x)\n",
        "    x = K.layers.Conv2D(128, 3, activation=\"linear\", use_bias=False)(x)\n",
        "    x = K.layers.BatchNormalization(axis=-1)(x)\n",
        "    x = K.layers.Activation(tf.nn.sigmoid)(x)\n",
        "    x = K.layers.Conv2D(64, 1, activation=\"linear\", use_bias=False)(x)\n",
        "    x = K.layers.BatchNormalization(axis=-1)(x)\n",
        "    x = K.layers.Activation(tf.nn.leaky_relu)(x)\n",
        "    x = K.layers.Conv2D(64, 3, activation=\"linear\", use_bias=False)(x)\n",
        "    x = K.layers.BatchNormalization(axis=-1)(x)\n",
        "    x = K.layers.Activation(tf.nn.leaky_relu)(x)\n",
        "    x = K.layers.Conv2D(1, 1, activation=\"linear\", use_bias=False)(x)\n",
        "    x = K.layers.BatchNormalization(axis=-1)(x)\n",
        "    x = K.layers.Activation(tf.nn.sigmoid)(x)\n",
        "    disc = K.models.Model(inputs=[conditioning, inputs], outputs=x)\n",
        "    return disc\n",
        "\n",
        "\n",
        "def create_generator_p2p():\n",
        "    inputs = K.layers.Input(shape=(128, 128, 1))\n",
        "    x = K.layers.BatchNormalization(axis=-1)(inputs)\n",
        "    x = K.layers.Conv2D(16, 3, activation=\"linear\", padding=\"SAME\", use_bias=False)(x)\n",
        "    x = K.layers.BatchNormalization(axis=-1)(x)\n",
        "    x = K.layers.Activation(tf.nn.leaky_relu)(x)\n",
        "    x = K.layers.Conv2D(16, 3, activation=\"linear\", padding=\"SAME\", use_bias=False)(x)\n",
        "    x = K.layers.BatchNormalization(axis=-1)(x)\n",
        "    x = K.layers.Activation(tf.nn.leaky_relu)(x)\n",
        "    x = K.layers.MaxPool2D(2)(x)\n",
        "    x = ResidualLayer([\n",
        "        K.layers.Conv2D(32, 3, activation=\"linear\", padding=\"SAME\", use_bias=False),\n",
        "        K.layers.BatchNormalization(axis=-1),\n",
        "        K.layers.Activation(tf.nn.leaky_relu),\n",
        "        K.layers.Conv2D(32, 3, activation=\"linear\", padding=\"SAME\", use_bias=False),\n",
        "        K.layers.BatchNormalization(axis=-1),\n",
        "        K.layers.Activation(tf.nn.leaky_relu),\n",
        "        K.layers.MaxPool2D(2),\n",
        "        ResidualLayer([\n",
        "            K.layers.Conv2D(64, 3, activation=\"linear\", padding=\"SAME\", use_bias=False),\n",
        "            K.layers.BatchNormalization(axis=-1),\n",
        "            K.layers.Activation(tf.nn.leaky_relu),\n",
        "            K.layers.Conv2D(64, 3, activation=\"linear\", padding=\"SAME\", use_bias=False),\n",
        "            K.layers.BatchNormalization(axis=-1),\n",
        "            K.layers.Activation(tf.nn.leaky_relu),\n",
        "            K.layers.MaxPool2D(2),\n",
        "            ResidualLayer([\n",
        "                K.layers.Conv2D(128, 3, activation=\"linear\", padding=\"SAME\", use_bias=False),\n",
        "                K.layers.BatchNormalization(axis=-1),\n",
        "                K.layers.Activation(tf.nn.leaky_relu),\n",
        "                K.layers.Conv2D(128, 3, activation=\"linear\", padding=\"SAME\", use_bias=False),\n",
        "                K.layers.BatchNormalization(axis=-1),\n",
        "                K.layers.Activation(tf.nn.leaky_relu),\n",
        "                K.layers.MaxPool2D(2),\n",
        "                ResidualLayer([\n",
        "                    K.layers.Conv2D(256, 1, activation=\"linear\", padding=\"SAME\", use_bias=False),\n",
        "                    K.layers.BatchNormalization(axis=-1),\n",
        "                    K.layers.Activation(tf.nn.leaky_relu),\n",
        "                    K.layers.Conv2D(256, 3, activation=\"linear\", padding=\"SAME\", use_bias=False),\n",
        "                    K.layers.BatchNormalization(axis=-1),\n",
        "                    K.layers.Activation(tf.nn.leaky_relu),\n",
        "                ]),\n",
        "                K.layers.UpSampling2D(2),\n",
        "                K.layers.Conv2D(128, 3, activation=\"linear\", padding=\"SAME\", use_bias=False),\n",
        "                K.layers.BatchNormalization(axis=-1),\n",
        "                K.layers.Activation(tf.nn.leaky_relu),\n",
        "                K.layers.Conv2D(128, 3, activation=\"linear\", padding=\"SAME\", use_bias=False),\n",
        "                K.layers.BatchNormalization(axis=-1),\n",
        "                K.layers.Activation(tf.nn.leaky_relu),\n",
        "            ]),\n",
        "            K.layers.UpSampling2D(2),\n",
        "            K.layers.Conv2D(64, 3, activation=\"linear\", padding=\"SAME\", use_bias=False),\n",
        "            K.layers.BatchNormalization(axis=-1),\n",
        "            K.layers.Activation(tf.nn.leaky_relu),\n",
        "            K.layers.Conv2D(64, 3, activation=\"linear\", padding=\"SAME\", use_bias=False),\n",
        "            K.layers.BatchNormalization(axis=-1),\n",
        "            K.layers.Activation(tf.nn.leaky_relu),\n",
        "        ]),\n",
        "        K.layers.UpSampling2D(2),\n",
        "        K.layers.Conv2D(32, 3, activation=\"linear\", padding=\"SAME\", use_bias=False),\n",
        "        K.layers.BatchNormalization(axis=-1),\n",
        "        K.layers.Activation(tf.nn.leaky_relu),\n",
        "        K.layers.Conv2D(32, 3, activation=\"linear\", padding=\"SAME\", use_bias=False),\n",
        "        K.layers.BatchNormalization(axis=-1),\n",
        "        K.layers.Activation(tf.nn.leaky_relu),\n",
        "    ])(x)\n",
        "    x = K.layers.UpSampling2D(2)(x)\n",
        "    x = K.layers.Conv2D(16, 3, activation=\"linear\", padding=\"SAME\", use_bias=False)(x)\n",
        "    x = K.layers.BatchNormalization(axis=-1)(x)\n",
        "    x = K.layers.Activation(tf.nn.leaky_relu)(x)\n",
        "    x = K.layers.Conv2D(16, 3, activation=\"linear\", padding=\"SAME\", use_bias=False)(x)\n",
        "    x = K.layers.BatchNormalization(axis=-1)(x)\n",
        "    x = K.layers.Activation(tf.nn.leaky_relu)(x)\n",
        "    x = K.layers.Conv2D(3, 3, activation=\"sigmoid\", padding=\"SAME\")(x)\n",
        "\n",
        "    unet = K.models.Model(inputs=inputs, outputs=x)\n",
        "    return unet\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    with open('drive/MyDrive/cache/X_train.npy', 'rb') as f:\n",
        "        X_train = np.load(f)\n",
        "    with open('drive/MyDrive/cache/X_val.npy', 'rb') as f:\n",
        "        X_val = np.load(f)\n",
        "    with open('drive/MyDrive/cache/X_test.npy', 'rb') as f:\n",
        "        X_test = np.load(f)\n",
        "\n",
        "    unet = create_generator_p2p()\n",
        "    patch_gan = create_discriminator_p2p()\n",
        "\n",
        "    unet.compile(\n",
        "        optimizer=tf.optimizers.legacy.Adam(1e-4, beta_1=0.5),\n",
        "        loss=tf.losses.MeanAbsoluteError()\n",
        "    )\n",
        "    patch_gan.compile(\n",
        "        optimizer=tf.optimizers.legacy.Adam(1e-4, beta_1=0.5)\n",
        "    )\n",
        "\n",
        "    train_pix2pix(X_train, unet, patch_gan, \"pix2pix-0.01\", alpha=0.01, save_only_weights=False,\n",
        "                  batch_size=8)\n"
      ],
      "metadata": {
        "id": "p-LhrNp0zNQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate images on test set and download it"
      ],
      "metadata": {
        "id": "4spmdyh80ON-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "# convert images in black and white\n",
        "X_test_gray = np.zeros_like(X_test[:, :, :, 0:1])\n",
        "\n",
        "for i in range(X_test.shape[0]):\n",
        "    gray_image = cv2.cvtColor(X_test[i], cv2.COLOR_BGR2GRAY)\n",
        "    gray_image = np.expand_dims(gray_image, axis=-1)\n",
        "    X_test_gray[i] = gray_image\n"
      ],
      "metadata": {
        "id": "5z4vmU7W0Q-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get model prediction on grey images (change autoencoder if you used another model)\n",
        "predictions = autoencoder.predict(X_test_gray)\n",
        "test_loss = autoencoder.evaluate(X_test_gray, X_test, verbose=1)"
      ],
      "metadata": {
        "id": "JPIlb9cq0u2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create a folder with the colorized images and saved images\n",
        "output_folder = 'generated_images'\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "for i, generated_image in enumerate(predictions):\n",
        "    # Denormalize the image that was previously normalized\n",
        "    generated_image_denormalized = (generated_image * 255.0).astype(np.uint8)\n",
        "\n",
        "    # Save image\n",
        "    filename = os.path.join(output_folder, f'generated_image_{i}.png')\n",
        "    plt.imsave(filename, generated_image_denormalized.squeeze())\n",
        "\n",
        "print(f\"Generated images saved in the folder: {output_folder}\")"
      ],
      "metadata": {
        "id": "8ZrwY_Hg09Fe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#download a zip file of the generated images folder\n",
        "import shutil\n",
        "\n",
        "folder_to_zip = 'generated_images'\n",
        "zip_destination = 'generated_images.zip'\n",
        "shutil.make_archive(zip_destination[:-4], 'zip', folder_to_zip)\n",
        "from google.colab import files\n",
        "files.download(zip_destination)"
      ],
      "metadata": {
        "id": "5ZDdNKpn1M1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metrics"
      ],
      "metadata": {
        "id": "qHvUr3921YcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_colourfulness(im):\n",
        "    \"\"\"\n",
        "    Calculate colourfulness in natural images.\n",
        "\n",
        "    Parameters:\n",
        "        im: ndarray\n",
        "            Image in RGB format.\n",
        "\n",
        "    Returns:\n",
        "        C: float\n",
        "            Colourfulness.\n",
        "    \"\"\"\n",
        "    im = im.astype(float)\n",
        "    R = im[:, :, 0]\n",
        "    G = im[:, :, 1]\n",
        "    B = im[:, :, 2]\n",
        "\n",
        "    # rg = |R - G|\n",
        "    rg = np.abs(R - G).flatten()\n",
        "\n",
        "    # yb = |0.5*(R + G) - B|\n",
        "    yb = np.abs(0.5 * (R + G) - B).flatten()\n",
        "\n",
        "    # Standard deviation and mean value of the pixel cloud along directions\n",
        "    std_RG = np.std(rg)\n",
        "    mean_RG = np.mean(rg)\n",
        "\n",
        "    std_YB = np.std(yb)\n",
        "    mean_YB = np.mean(yb)\n",
        "\n",
        "    std_RGYB = np.sqrt(std_RG*2 + std_YB*2)\n",
        "    mean_RGYB = np.sqrt(mean_RG*2 + mean_YB*2)\n",
        "\n",
        "    C = std_RGYB + (0.3 * mean_RGYB)\n",
        "\n",
        "    return C"
      ],
      "metadata": {
        "id": "6vCGIWe21aD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload foders with generated images and original test images as zip file then use this code to unzip them\n",
        "from zipfile import ZipFile\n",
        "import os\n",
        "\n",
        "#generated images\n",
        "zip_file_path = 'generated_images.zip'\n",
        "extracted_folder_path = 'content/images'\n",
        "\n",
        "with ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_folder_path)\n",
        "\n",
        "#original test images\n",
        "zip_file_path = 'test_images.zip'\n",
        "extracted_folder_path = 'content/test_images'\n",
        "with ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_folder_path)"
      ],
      "metadata": {
        "id": "KfhZkEjv1bm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get colorfullness of all images\n",
        "\n",
        "folder_path = 'content/images/'\n",
        "image_files = os.listdir(folder_path)\n",
        "count=0\n",
        "colorfullnes=0\n",
        "for image_file in image_files:\n",
        "    image_path = os.path.join(folder_path, image_file)\n",
        "    image = Image.open(image_path)\n",
        "\n",
        "    # If the image has 4 channels convert it into rgb\n",
        "    if image.mode == 'RGBA':\n",
        "        image = image.convert('RGB')\n",
        "\n",
        "    image_array = np.array(image)\n",
        "    colorfullnes+=get_colourfulness(image_array)\n",
        "    count+=1\n",
        "\n",
        "#print colorfullness and number of images\n",
        "print(colorfullnes)\n",
        "print(count)"
      ],
      "metadata": {
        "id": "GfDbuqbO14BM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define psnr\n",
        "def psnr(original, compressed):\n",
        "    \"\"\"\n",
        "    Calculate PSNR (Peak Signal-to-Noise Ratio) between two images.\n",
        "\n",
        "    Parameters:\n",
        "        original: ndarray\n",
        "            Original image.\n",
        "        compressed: ndarray\n",
        "            Compressed image.\n",
        "\n",
        "    Returns:\n",
        "        psnr_value: float\n",
        "            PSNR value.\n",
        "    \"\"\"\n",
        "    mse = np.mean((original - compressed) ** 2)\n",
        "    max_pixel = np.max(original)\n",
        "    psnr_value = 20 * np.log10(max_pixel / np.sqrt(mse))\n",
        "    return psnr_value"
      ],
      "metadata": {
        "id": "yjuyamqt2P9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#psnr of all images\n",
        "psnr_tot=0\n",
        "count=0\n",
        "\n",
        "\n",
        "folder_path = 'content/images/'\n",
        "test_images_path = 'content/test_images/test_images/'\n",
        "png_image_files = [file for file in os.listdir(folder_path) if file.lower().endswith('.png')]\n",
        "\n",
        "for png_image_file in png_image_files:\n",
        "    image_number = int(png_image_file.split(\"_\")[-1].split(\".\")[0])\n",
        "    png_image_path = os.path.join(folder_path, png_image_file)\n",
        "    png_image = Image.open(png_image_path)\n",
        "\n",
        "    # If the image has 4 channels convert it into rgb\n",
        "    if png_image.mode == 'RGBA':\n",
        "      png_image = image.convert('RGB')\n",
        "    png_image_array = np.array(png_image)\n",
        "    test_image_path = os.path.join(test_images_path, f'image_{image_number + 1}.jpg')\n",
        "    test_image = Image.open(test_image_path)\n",
        "    test_image_array = np.array(test_image)\n",
        "\n",
        "    psnr_tot+=psnr(test_image_array, png_image_array)\n",
        "    count+=1\n",
        "\n",
        "#print value\n",
        "print(psnr_tot)\n",
        "print(count)"
      ],
      "metadata": {
        "id": "gG5tllyA2XCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#definte the function to compute ssim\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "def calculate_ssim(original, compressed):\n",
        "    \"\"\"\n",
        "    Calculate SSIM (Structural Similarity Index) between two images.\n",
        "\n",
        "    Parameters:\n",
        "        original: ndarray\n",
        "            Original image.\n",
        "        compressed: ndarray\n",
        "            Compressed image.\n",
        "\n",
        "    Returns:\n",
        "        ssim_value: float\n",
        "            SSIM value.\n",
        "    \"\"\"\n",
        "    ssim_value, _ = ssim(original, compressed, win_size=3, full=True)\n",
        "    return ssim_value"
      ],
      "metadata": {
        "id": "2ZgjO6jY2pD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compute ssim on all images\n",
        "\n",
        "ssim_tot=0\n",
        "count=0\n",
        "folder_path = 'content/images/'\n",
        "test_images_path = 'content/test_images/test_images/'\n",
        "png_image_files = [file for file in os.listdir(folder_path) if file.lower().endswith('.png')]\n",
        "\n",
        "for png_image_file in png_image_files:\n",
        "    image_number = int(png_image_file.split(\"_\")[-1].split(\".\")[0])\n",
        "    png_image_path = os.path.join(folder_path, png_image_file)\n",
        "    png_image = Image.open(png_image_path)\n",
        "\n",
        "    if png_image.mode == 'RGBA':\n",
        "      png_image = image.convert('RGB')\n",
        "\n",
        "    png_image_array = np.array(png_image)\n",
        "    test_image_path = os.path.join(test_images_path, f'image_{image_number + 1}.jpg')\n",
        "    test_image = Image.open(test_image_path)\n",
        "    test_image_array = np.array(test_image)\n",
        "    ssim_couple=calculate_ssim(test_image_array, png_image_array)\n",
        "    ssim_tot+=ssim_couple\n",
        "    count+=1\n",
        "\n",
        "#print results\n",
        "print(ssim_tot)\n",
        "print(count)\n",
        "\n"
      ],
      "metadata": {
        "id": "btiSYW93201I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mean squared error\n",
        "\n",
        "mse_tot=0\n",
        "count=0\n",
        "folder_path = 'content/images/'\n",
        "test_images_path = 'content/test_images/test_images/'\n",
        "png_image_files = [file for file in os.listdir(folder_path) if file.lower().endswith('.png')]\n",
        "for png_image_file in png_image_files:\n",
        "    image_number = int(png_image_file.split(\"_\")[-1].split(\".\")[0])\n",
        "    png_image_path = os.path.join(folder_path, png_image_file)\n",
        "    png_image = Image.open(png_image_path)\n",
        "\n",
        "    if png_image.mode == 'RGBA':\n",
        "      png_image = image.convert('RGB')\n",
        "\n",
        "    png_image_array = np.array(png_image)\n",
        "    test_image_path = os.path.join(test_images_path, f'image_{image_number + 1}.jpg')\n",
        "    test_image = Image.open(test_image_path)\n",
        "    test_image_array = np.array(test_image)\n",
        "    mse_tot += np.sum((test_image_array - png_image_array) ** 2)\n",
        "    count+=1\n",
        "\n",
        "# Compute the mean of the squared errors\n",
        "mean_mse=mse_tot/count\n",
        "\n",
        "# Print results\n",
        "print(mean_mse)\n",
        "print(count)"
      ],
      "metadata": {
        "id": "MYCdoWW13I69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#mean absolute errore\n",
        "mae_tot=0\n",
        "count=0\n",
        "folder_path = 'content/images/'\n",
        "test_images_path = 'content/test_images/test_images/'\n",
        "png_image_files = [file for file in os.listdir(folder_path) if file.lower().endswith('.png')]\n",
        "\n",
        "for png_image_file in png_image_files:\n",
        "    image_number = int(png_image_file.split(\"_\")[-1].split(\".\")[0])\n",
        "    png_image_path = os.path.join(folder_path, png_image_file)\n",
        "    png_image = Image.open(png_image_path)\n",
        "\n",
        "    if png_image.mode == 'RGBA':\n",
        "      png_image = image.convert('RGB')\n",
        "\n",
        "    png_image_array = np.array(png_image)\n",
        "    test_image_path = os.path.join(test_images_path, f'image_{image_number + 1}.jpg')\n",
        "    test_image = Image.open(test_image_path)\n",
        "    test_image_array = np.array(test_image)\n",
        "    mae_tot += np.sum(np.abs(test_image_array - png_image_array))\n",
        "    count+=1\n",
        "\n",
        "# Compute the mean MAE\n",
        "mean_mae=mae_tot/count\n",
        "#print results\n",
        "print(mean_mae)\n",
        "print(count)"
      ],
      "metadata": {
        "id": "vDpqUhml3bCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot"
      ],
      "metadata": {
        "id": "AlMr8Hvx3sIP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I upload a folder with selected images for i from 1 to 10 named test_i for the test and autoencoder_i, unet_i, dense_net_i, etc. for the models"
      ],
      "metadata": {
        "id": "U7GPDPgf3uGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os"
      ],
      "metadata": {
        "id": "LFmXJJ0-3ntn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(8, 10, figsize=(13,12), dpi=200)\n",
        "for ax in axs.flatten():\n",
        "    ax.set_yticks([], [])\n",
        "    ax.set_xticks([], [])\n",
        "axs[0, 0].set_ylabel(\"Grayscale\\n128x128\", rotation=0, fontsize=15, labelpad=60)\n",
        "axs[1, 0].set_ylabel(\"Ground truth\\n128x128\", rotation=0, fontsize=15, labelpad=60)\n",
        "axs[2, 0].set_ylabel(\"Autoencoder\\n128x128\", rotation=0, fontsize=15, labelpad=60)\n",
        "axs[3, 0].set_ylabel(\"Deep CNN\\n128x128\", rotation=0, fontsize=15, labelpad=60)\n",
        "axs[4, 0].set_ylabel(\"U-Net\\n128x128\", rotation=0, fontsize=15, labelpad=60)\n",
        "axs[5, 0].set_ylabel(\"ResNet\\n128x128\", rotation=0, fontsize=15, labelpad=60)\n",
        "axs[6, 0].set_ylabel(\"DenseNet\\n128x128\", rotation=0, fontsize=15, labelpad=60)\n",
        "axs[7, 0].set_ylabel(\"Pix2Pix\\n128x128\", rotation=0, fontsize=15, labelpad=60)\n",
        "for i in range(10):\n",
        "    axs[0, i].imshow(Image.open(''.join([\"/content/immagini/immagini_finali/test_\", str(i+1),\".jpg\" ])).convert(\"L\"), cmap=\"gray\")\n",
        "    axs[1, i].imshow(Image.open(''.join([\"/content/immagini/immagini_finali/test_\", str(i+1),\".jpg\" ])))\n",
        "    axs[2, i].imshow(Image.open(''.join([\"/content/immagini/immagini_finali/autoencoder_\", str(i+1),\".png\" ])))\n",
        "    axs[3, i].imshow(Image.open(''.join([\"/content/immagini/immagini_finali/deep_nn_\", str(i+1),\".png\" ])))\n",
        "    axs[4, i].imshow(Image.open(''.join([\"/content/immagini/immagini_finali/unet_\", str(i+1),\".png\" ])))\n",
        "    axs[5, i].imshow(Image.open(''.join([\"/content/immagini/immagini_finali/resnet_\", str(i+1),\".png\" ])))\n",
        "    axs[6, i].imshow(Image.open(''.join([\"/content/immagini/immagini_finali/dense_net_\", str(i+1),\".png\" ])))\n",
        "    axs[7, i].imshow(Image.open(''.join([\"/content/immagini/immagini_finali/pix2pix_\", str(i+1),\".png\" ])))\n",
        "print(\"finish\")\n",
        "plt.subplots_adjust(wspace=0.1, hspace=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4FyeEN9W4GGt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}